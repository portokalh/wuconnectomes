{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dipy.io.streamline import load_trk, save_trk\n",
    "from dipy.viz import window, actor\n",
    "import os\n",
    "import pickle\n",
    "from tract_visualize import show_bundles, setup_view\n",
    "from convert_atlas_mask import convert_labelmask, atlas_converter\n",
    "from tract_handler import ratio_to_str, gettrkpath\n",
    "from itertools import compress\n",
    "import numpy as np\n",
    "import nibabel as nib, socket\n",
    "from file_tools import mkcdir\n",
    "from streamline_nocheck import load_trk as load_trk_spe\n",
    "from dipy.segment.clustering import QuickBundles\n",
    "from dipy.segment.metric import ResampleFeature, AveragePointwiseEuclideanMetric\n",
    "import warnings\n",
    "from dipy.align.streamlinear import StreamlineLinearRegistration\n",
    "import copy\n",
    "\n",
    "\"\"\"\n",
    "# ,(23,30)\n",
    "target_tuples = [(9, 1), (24, 1), (22, 1), (58, 57), (64, 57)]\n",
    "target_tuples = [(9, 1), (24, 1), (22, 1), (58, 57), (23, 24), (64, 57)]\n",
    "target_tuples = [(58, 57), (9, 1), (24, 1), (22, 1), (64, 57), (23, 24), (24, 30), (23, 30)]\n",
    "target_tuples = [(24, 30), (23, 24)]\n",
    "target_tuples = [(80, 58)]\n",
    "\"\"\"\n",
    "\n",
    "computer_name = socket.gethostname()\n",
    "\n",
    "project = 'AD_Decode'\n",
    "\n",
    "fixed = True\n",
    "record = ''\n",
    "\n",
    "inclusive = False\n",
    "symmetric = True\n",
    "write_txt = True\n",
    "ratio = 100\n",
    "top_percentile = 100\n",
    "num_bundles = 20\n",
    "\n",
    "selection = 'num_streams'\n",
    "coloring = 'bundles_coloring'\n",
    "references = ['fa','md']\n",
    "references = ['fa']\n",
    "cutoffref = 0\n",
    "\n",
    "write_stats = False\n",
    "registration = False\n",
    "overwrite = True\n",
    "\n",
    "#genotype_noninclusive\n",
    "#target_tuples = [(9, 1), (24, 1), (58, 57), (64, 57), (22, 1)]\n",
    "#genotype_noninclusive_volweighted_fa\n",
    "#target_tuples = [(9, 1), (57, 9), (61, 23), (84, 23), (80, 9)]\n",
    "\n",
    "#sex_noninclusive\n",
    "#target_tuples = [(64, 57), (58, 57), (9, 1), (64, 58), (80,58)]\n",
    "#target_tuples = [(64,57)]\n",
    "#sex_noninclusive_volweighted_fa\n",
    "#target_tuples = [(58, 24), (58, 30), (64, 30), (64, 24), (58,48)]\n",
    "\n",
    "#target_tuples = [(9,1)]\n",
    "#target_tuples = [(9,1)]\n",
    "\n",
    "#groups = ['APOE4', 'APOE3']\n",
    "\n",
    "changewindow_eachtarget = False\n",
    "\n",
    "if inclusive:\n",
    "    inclusive_str = '_inclusive'\n",
    "else:\n",
    "    inclusive_str = '_non_inclusive'\n",
    "\n",
    "if symmetric:\n",
    "    symmetric_str = '_symmetric'\n",
    "else:\n",
    "    symmetric_str = '_non_symmetric'\n",
    "\n",
    "# if fixed:\n",
    "#    fixed_str = '_fixed'\n",
    "# else:\n",
    "#    fixed_str = ''\n",
    "\n",
    "samos = False\n",
    "if 'samos' in computer_name:\n",
    "    mainpath = '/mnt/paros_MRI/jacques/'\n",
    "    ROI_legends = \"/mnt/paros_MRI/jacques/atlases/IITmean_RPI/IITmean_RPI_index.xlsx\"\n",
    "elif 'santorini' in computer_name:\n",
    "    # mainpath = '/Users/alex/jacques/'\n",
    "    mainpath = '/Volumes/Data/Badea/Lab/human/'\n",
    "    ROI_legends = \"/Volumes/Data/Badea/ADdecode.01/Analysis/atlases/IITmean_RPI/IITmean_RPI_index.xlsx\"\n",
    "elif 'blade' in computer_name:\n",
    "    mainpath = '/mnt/munin6/Badea/Lab/human/'\n",
    "    ROI_legends = \"/mnt/munin6/Badea/Lab/atlases/IITmean_RPI/IITmean_RPI_index.xlsx\"\n",
    "else:\n",
    "    raise Exception('No other computer name yet')\n",
    "\n",
    "# target_tuple = (24,1)\n",
    "# target_tuple = [(58, 57)]\n",
    "# target_tuples = [(64, 57)]\n",
    "\n",
    "\n",
    "ratio_str = ratio_to_str(ratio)\n",
    "print(ratio_str)\n",
    "if ratio_str == '_all':\n",
    "    folder_ratio_str = ''\n",
    "else:\n",
    "    folder_ratio_str = ratio_str.replace('_ratio', '')\n",
    "# target_tuple = (9,77)\n",
    "\n",
    "_, _, index_to_struct, _ = atlas_converter(ROI_legends)\n",
    "\n",
    "if project == 'AMD':\n",
    "    mainpath = os.path.join(mainpath, project)\n",
    "    groups = ['Initial AMD', 'Paired 2-YR AMD', 'Initial Control', 'Paired 2-YR Control', 'Paired Initial Control',\n",
    "              'Paired Initial AMD']\n",
    "    anat_path = '/Volumes/Data/Badea/Lab/mouse/VBM_19BrainChAMD01_IITmean_RPI_with_2yr-work/dwi/SyN_0p5_3_0p5_dwi/dwiMDT_Control_n72_i6/median_images/MDT_dwi.nii.gz'\n",
    "\n",
    "if project == 'AD_Decode':\n",
    "    mainpath = os.path.join(mainpath, project, 'Analysis')\n",
    "    anat_path = '/Volumes/Data/Badea/Lab/mouse/VBM_21ADDecode03_IITmean_RPI_fullrun-work/dwi/SyN_0p5_3_0p5_fa/faMDT_NoNameYet_n37_i6/median_images/MDT_b0.nii.gz'\n",
    "\n",
    "# figures_path = '/Volumes/Data/Badea/Lab/human/AMD/Figures_MDT_non_inclusive/'\n",
    "# centroid_folder = '/Volumes/Data/Badea/Lab/human/AMD/Centroids_MDT_non_inclusive/'\n",
    "figures_path = os.path.join(mainpath, f'Figures_MDT{inclusive_str}{symmetric_str}{folder_ratio_str}')\n",
    "centroid_folder = os.path.join(mainpath, f'Centroids_MDT{inclusive_str}{symmetric_str}{folder_ratio_str}')\n",
    "trk_folder = os.path.join(mainpath, f'Centroids_MDT{inclusive_str}{symmetric_str}{folder_ratio_str}')\n",
    "stats_folder = os.path.join(mainpath, f'Statistics_MDT{inclusive_str}{symmetric_str}{folder_ratio_str}')\n",
    "\n",
    "mkcdir([figures_path, centroid_folder, stats_folder])\n",
    "\n",
    "# groups = ['Initial AMD', 'Paired 2-YR AMD', 'Initial Control', 'Paired 2-YR Control', 'Paired Initial Control',\n",
    "#          'Paired Initial AMD']\n",
    "\n",
    "# anat_path = '/Volumes/Data/Badea/Lab/mouse/VBM_19BrainChAMD01_IITmean_RPI_with_2yr-work/dwi/SyN_0p5_3_0p5_dwi/dwiMDT_Control_n72_i6/median_images/MDT_dwi.nii.gz'\n",
    "\n",
    "\n",
    "# superior frontal right to cerebellum right\n",
    "\n",
    "#set parameter\n",
    "num_points1 = 50\n",
    "distance1 = 1\n",
    "feature1 = ResampleFeature(nb_points=num_points1)\n",
    "metric1 = AveragePointwiseEuclideanMetric(feature=feature1)\n",
    "\n",
    "#group cluster parameter\n",
    "num_points2 = 50\n",
    "distance2 = 2\n",
    "feature2 = ResampleFeature(nb_points=num_points2)\n",
    "metric2 = AveragePointwiseEuclideanMetric(feature=feature2)\n",
    "\n",
    "scene = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groups = ['Male','Female']\n",
    "groups = ['APOE4', 'APOE3']\n",
    "non_control = groups[0]\n",
    "control = groups[1]\n",
    "\n",
    "#genotype_noninclusive\n",
    "#target_tuples = [(9, 1), (24, 1), (58, 57), (64, 57), (22, 1)]\n",
    "#genotype_noninclusive_volweighted_fa\n",
    "#target_tuples = [(9, 1), (57, 9), (61, 23), (84, 23), (80, 9)]\n",
    "\n",
    "#sex_noninclusive\n",
    "#target_tuples = [(64, 57), (58, 57), (9, 1), (64, 58), (80,58)]\n",
    "#target_tuples = [(64,57)]\n",
    "#sex_noninclusive_volweighted_fa\n",
    "#target_tuples = [(58, 24), (58, 30), (64, 30), (64, 24), (58,48)]\n",
    "\n",
    "target_tuple = (9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(target_tuple[0], target_tuple[1])\n",
    "region_connection = index_to_struct[target_tuple[0]] + '_to_' + index_to_struct[target_tuple[1]]\n",
    "print(region_connection)\n",
    "\n",
    "if write_txt:\n",
    "    text_path = os.path.join(figures_path, region_connection + '_stats.txt')\n",
    "    testfile = open(text_path, \"w\")\n",
    "    testfile.write(\"Parameters for groups\\n\")\n",
    "    testfile.close()\n",
    "\n",
    "if changewindow_eachtarget:\n",
    "    firstrun = True\n",
    "\n",
    "selected_bundles = {}\n",
    "selected_centroids = {}\n",
    "selected_sizes = {}\n",
    "streamlines = {}\n",
    "num_bundles_group = {}\n",
    "\n",
    "ref_lines = {}\n",
    "ref_points = {}\n",
    "\n",
    "for group in groups:\n",
    "\n",
    "    selected_bundles[group] = []\n",
    "    selected_centroids[group] = []\n",
    "    selected_sizes[group] = []\n",
    "\n",
    "    print(f'Setting up group {group}')\n",
    "    group_str = group.replace(' ', '_')\n",
    "\n",
    "    stats_path = os.path.join(stats_folder,\n",
    "                              group_str + '_MDT' + ratio_str + '_' + region_connection + '_bundle_stats.xlsx')\n",
    "\n",
    "    if not os.path.exists(stats_path) or overwrite:\n",
    "        if os.path.exists(stats_path):\n",
    "            os.remove(stats_path)\n",
    "        import xlsxwriter\n",
    "        workbook = xlsxwriter.Workbook(stats_path)\n",
    "        worksheet = workbook.add_worksheet()\n",
    "        l=1\n",
    "        worksheet.write(0,l,'Number streamlines')\n",
    "        l+=1\n",
    "        for ref in references:\n",
    "            worksheet.write(0,l, ref + ' mean')\n",
    "            worksheet.write(0,l+1, ref + ' min')\n",
    "            worksheet.write(0,l+2, ref + ' max')\n",
    "            worksheet.write(0,l+3, ref + ' std')\n",
    "            l=l+4\n",
    "    else:\n",
    "        print(f'The file {stats_path} already exists and no overwrite enabled: skipping')\n",
    "        #continue\n",
    "\n",
    "    centroid_file_path = os.path.join(centroid_folder,\n",
    "                                      group_str + '_MDT' + ratio_str + '_' + region_connection + '_centroid.py')\n",
    "\n",
    "    trk_path = os.path.join(trk_folder,\n",
    "                            group_str + '_MDT' + ratio_str + '_' + region_connection + '_streamlines.trk')\n",
    "\n",
    "    # '/Volumes/Data/Badea/Lab/human/AD_Decode/Analysis/Centroids_MDT_non_inclusive_symmetric_100/APOE4_MDT_ratio_100_ctx-lh-inferiorparietal_left_to_ctx-lh-inferiortemporal_left_streamlines.trk'\n",
    "    if os.path.exists(trk_path):\n",
    "        try:\n",
    "            streamlines_data = load_trk(trk_path, 'same')\n",
    "        except:\n",
    "            streamlines_data = load_trk_spe(trk_path, 'same')\n",
    "    streamlines[group] = streamlines_data.streamlines\n",
    "\n",
    "\n",
    "    for ref in references:\n",
    "        ref_path_lines = os.path.join(centroid_folder,\n",
    "                               group_str + '_MDT' + ratio_str + '_' + region_connection + f'_{ref}_lines.py')\n",
    "        ref_path_points = os.path.join(centroid_folder,\n",
    "                               group_str + '_MDT' + ratio_str + '_' + region_connection + f'_{ref}_points.py')\n",
    "\n",
    "        if os.path.exists(ref_path_points):\n",
    "            with open(ref_path_points, 'rb') as f:\n",
    "                ref_points[group,ref] = pickle.load(f)\n",
    "        else:\n",
    "            txt = f'Could not find file {ref_path_points} for group {group} reference {ref}'\n",
    "            raise Exception(txt)\n",
    "\n",
    "        if os.path.exists(ref_path_lines):\n",
    "            with open(ref_path_lines, 'rb') as f:\n",
    "                ref_lines[group,ref] = pickle.load(f)\n",
    "        else:\n",
    "            txt = f'Could not find file {ref_path_lines} for group {group} reference {ref}'\n",
    "            raise Exception(txt)\n",
    "\n",
    "\n",
    "\n",
    "    if top_percentile<100:\n",
    "        cutoff = np.percentile(ref_lines[group,references[cutoffref]], 100 - top_percentile)\n",
    "        select_streams = ref_lines[group,references[cutoffref]] > cutoff\n",
    "        streamlines[group] = list(compress(streamlines[group], select_streams))\n",
    "        streamlines[group] = nib.streamlines.ArraySequence(streamlines[group])\n",
    "\n",
    "        for ref in references:\n",
    "            if np.shape(streamlines[group])[0] != np.shape(ref_lines[group][ref])[0]:\n",
    "                raise Exception('Inconsistency between streamlines and fa lines')\n",
    "            ref_lines[group,ref] = list(compress(ref_lines[group,ref], select_streams))\n",
    "\n",
    "    group_qb = QuickBundles(threshold=distance2, metric=metric2)\n",
    "    group_clusters = group_qb.cluster(streamlines[group])\n",
    "    #group2_qb = QuickBundles(threshold=distance2, metric=metric2)\n",
    "    #group2_clusters = group2_qb.cluster(groupstreamlines2)\n",
    "\n",
    "    num_bundles_group[group]=0\n",
    "    if selection =='num_streams':\n",
    "        num_streamlines = [np.shape(cluster)[0] for cluster in group_clusters.clusters]\n",
    "        num_streamlines = group_clusters.clusters_sizes()\n",
    "        top_bundles = sorted(range(len(num_streamlines)), key=lambda i: num_streamlines[i], reverse=True)[:]\n",
    "    for bundle in top_bundles:\n",
    "        selected_bundles[group].append(group_clusters.clusters[bundle])\n",
    "        selected_centroids[group].append(group_clusters.centroids[bundle])\n",
    "        selected_sizes[group].append(group_clusters.clusters_sizes()[bundle])\n",
    "        num_bundles_group[group]+=1\n",
    "    bun_num = 0\n",
    "\n",
    "    bundles_ref = {}\n",
    "    bundles_ref_mean = {}\n",
    "    for ref in references:\n",
    "        bundles_ref[ref] = []\n",
    "        bundles_ref_mean[ref] = []\n",
    "\n",
    "    for bundle in selected_bundles[group]:\n",
    "        for ref in references:\n",
    "            bundle_ref = []\n",
    "            for idx in bundle.indices:\n",
    "                bundle_ref.append(ref_lines[group,ref][idx])\n",
    "            bundles_ref[ref].append(bundle_ref)\n",
    "            bundles_ref_mean[ref].append(np.mean(bundle_ref))\n",
    "\n",
    "    empty_bundles = {}\n",
    "    for ref in references:\n",
    "        empty_bundles[ref] = 0\n",
    "\n",
    "    if write_stats:\n",
    "        bun_num=0\n",
    "        for bundle in top_bundles:\n",
    "            l=0\n",
    "            worksheet.write(bun_num+1, l, bun_num+1)\n",
    "            l+=1\n",
    "            worksheet.write(bun_num + 1, l, np.shape(group_clusters.clusters[bundle])[0])\n",
    "            l+=1\n",
    "            for ref in references:\n",
    "                if np.mean(bundles_ref[ref][bun_num])==0:\n",
    "                    empty_bundles[ref] += 1\n",
    "                worksheet.write(bun_num+1, l+0, np.mean(bundles_ref[ref][bun_num]))\n",
    "                worksheet.write(bun_num+1, l+1, np.min(bundles_ref[ref][bun_num]))\n",
    "                worksheet.write(bun_num+1, l+2, np.max(bundles_ref[ref][bun_num]))\n",
    "                worksheet.write(bun_num+1, l+3, np.std(bundles_ref[ref][bun_num]))\n",
    "                l = l + 4\n",
    "            bun_num+=1\n",
    "        workbook.close()\n",
    "    for ref in references:\n",
    "        if empty_bundles[ref]>0:\n",
    "            print(f'Found {empty_bundles} empty bundles out of {np.size(top_bundles)} for {ref} in group {group} for {region_connection}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if registration:\n",
    "    srr = StreamlineLinearRegistration()\n",
    "    for streamline,i in enumerate(selected_centroids[non_control]):\n",
    "        srm = srr.optimize(static=selected_centroids[control], moving=streamline)\n",
    "        streamlines[control][i] = srm.transform(streamline)\n",
    "\n",
    "from dipy.segment.metric import ResampleFeature, AveragePointwiseEuclideanMetric, mdf\n",
    "\n",
    "num_bundles = 20\n",
    "#dist_all = np.zeros((np.size(selected_bundles[control]), np.size(selected_bundles[non_control])))\n",
    "dist_all = np.zeros((num_bundles, num_bundles))\n",
    "\n",
    "for g3 in np.arange(num_bundles):\n",
    "    for g4 in np.arange(num_bundles):\n",
    "        dist_all[g3, g4] = (mdf(selected_centroids[control][g3], selected_centroids[non_control][g4]))\n",
    "\n",
    "dist_all_fix = copy.copy(dist_all)\n",
    "dist_all_idx = []\n",
    "#for i in range(len(selected_centroids[group])):\n",
    "for i in np.arange(num_bundles):\n",
    "    idx = np.argmin(dist_all_fix[i, :])\n",
    "    dist_all_idx.append([i, idx])\n",
    "    dist_all_fix[:, idx] = 100000\n",
    "\n",
    "dist_group3_idx = [dist_all_idx[iii][0] for iii in range(num_bundles)]  # size id\n",
    "dist_group4_idx = [dist_all_idx[iii][1] for iii in range(num_bundles)]  # size id\n",
    "\n",
    "group_list = {}\n",
    "dist_idx = {}\n",
    "for j,group in enumerate(groups):\n",
    "    dist_idx[group] = [dist_all_idx[iii][j] for iii in range(num_bundles)]\n",
    "    group_list[group]=([np.arange(num_bundles)[dist_all_idx[i][j]] for i in range(num_bundles)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from dipy.tracking import utils\n",
    "\n",
    "\n",
    "for group in groups:\n",
    "    groupcsv = np.zeros((1, 6))\n",
    "    for i in range(6):\n",
    "        idsize = dist_group3_idx[i]\n",
    "        idbundle = group3List[i]\n",
    "        fa = []\n",
    "        for s in selected_bundles[group][idbundle].indices:\n",
    "            #temp = np.hstack((idsize * np.ones((num_points2, 1)),\n",
    "            #                  idbundle * np.ones((num_points2, 1)),\n",
    "            #                  s * np.ones((num_points2, 1)),\n",
    "            #                  np.array(range(num_points2)).reshape(num_points2, 1),\n",
    "            #                  list(utils.length([streamlines[group][s]])) * np.ones((num_points2, 1)),\n",
    "            #                  np.array(ref_points[group, ref][s]).reshape(num_points2, 1)))\n",
    "            temp = np.hstack((idsize * np.ones((num_points2, 1)),\n",
    "                              idbundle * np.ones((num_points2, 1)),\n",
    "                              s * np.ones((num_points2, 1)),\n",
    "                              np.array(range(num_points2)).reshape(num_points2, 1),\n",
    "                              list(utils.length([streamlines[group][s]])) * np.ones((num_points2, 1))))\n",
    "            for ref in references:\n",
    "                np.hstack((temp,np.array(ref_points[group, ref][s]).reshape(num_points2, 1)))\n",
    "            groupcsv = np.vstack((groupcsv, temp))\n",
    "    groupcsv = groupcsv[1:, :]\n",
    "    groupcsvDF = pd.DataFrame(groupcsv)\n",
    "    groupcsvDF.rename(index=str, columns={0: \"Bundle Size Rank\", 1: \"Bundle ID\", 2: \"Steamlines ID\",\n",
    "                                           3: \"Point ID\", 4: \"length\"})\n",
    "    for ref,i in enumerate(references):\n",
    "        groupcsvDF.rename(index=str, columns={5+i: ref})\n",
    "    references_string = \"_\".join(references)\n",
    "    csv_summary = os.path.join(stats_folder, group + '_' + region_connection + f'_bundle_stats_{references_string}.csv')\n",
    "\n",
    "    groupcsvDF.to_csv(csv_summary, header=[\"Bundle Size Rank\", \"Bundle ID\", \"Streamlines ID\",\n",
    "                                 \"Point ID\", \"FA\", \"Length\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
